{"prompt": "What is RAG?", "response": "RAG (Retrieval-Augmented Generation) combines retrieval of relevant documents with LLM generation to improve accuracy. It first retrieves relevant context from a knowledge base, then uses that context to generate more accurate and informed responses."}
{"prompt": "How to fine-tune an LLM?", "response": "Use LoRA (Low-Rank Adaptation) to efficiently fine-tune large models on custom data. LoRA adds trainable low-rank matrices to the model's attention layers, allowing adaptation with minimal parameters and compute."}
{"prompt": "What is quantization in deep learning?", "response": "Quantization reduces model precision from 32-bit floats to lower bit widths (e.g., 4-bit or 8-bit) to decrease memory usage and speed up inference. 4-bit quantization can reduce memory by 75% while maintaining reasonable performance."}
{"prompt": "What is LoRA?", "response": "LoRA (Low-Rank Adaptation) is a parameter-efficient fine-tuning method. Instead of updating all model parameters, LoRA adds small trainable matrices to the attention layers, making fine-tuning much more efficient."}
{"prompt": "How does vector search work?", "response": "Vector search converts text into dense vector embeddings using neural networks, then finds similar vectors using approximate nearest neighbor search (e.g., FAISS). Similar vectors correspond to semantically similar text."}
{"prompt": "What is FAISS?", "response": "FAISS (Facebook AI Similarity Search) is a library for efficient similarity search and clustering of dense vectors. It supports both CPU and GPU implementations and is commonly used for RAG systems."}
{"prompt": "What is the difference between RAG and fine-tuning?", "response": "RAG retrieves external knowledge at inference time, while fine-tuning adapts the model's weights during training. RAG provides up-to-date information, while fine-tuning improves the model's understanding of domain-specific patterns."}
{"prompt": "How do you chunk documents for RAG?", "response": "Documents are split into overlapping chunks (typically 500-2000 tokens) to balance context size with retrieval precision. Overlapping helps preserve context across chunk boundaries. Common approaches use recursive text splitting."}

